{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom_wavelet_transform.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPrwAFTaLykActymMf1ACXR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/advaitkumar3107/Speech-Denoising-Using-Deep-Learning/blob/master/VGG-16_with_wavelet_transform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04_gzNMnb0AX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "import os\n",
        "import glob\n",
        "import sys\n",
        "import scipy\n",
        "import random\n",
        "from PIL import Image\n",
        "from torch.nn import init\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import librosa\n",
        "import librosa.display\n",
        "from tqdm import tqdm_notebook\n",
        "from scipy import signal\n",
        "from scipy.io.wavfile import read, write\n",
        "from numpy.fft import fft, ifft\n",
        "from google.colab import drive\n",
        "from torch.autograd import Variable\n",
        "from IPython.display import Audio\n",
        "import pywt\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/My\\ Drive/sample_audio_dataset\n",
        "torch.cuda.manual_seed(7)\n",
        "torch.manual_seed(7)\n",
        "np.random.seed(7)\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DNr6otbiWGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        init.normal(m.weight.data, 0.0, 0.02)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        init.normal(m.weight.data, 0.0, 0.02)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        init.normal(m.weight.data, 1.0, 0.02)\n",
        "        init.constant(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_xavier(m):\n",
        "  classname = m.__class__.__name__\n",
        "  #print(classname)\n",
        "  if isinstance(m, nn.Conv2d):\n",
        "      init.xavier_normal(m.weight.data, gain=1)\n",
        "  elif isinstance(m, nn.Linear):\n",
        "      init.xavier_normal(m.weight.data, gain=1)\n",
        "  elif isinstance(m, nn.BatchNorm2d):\n",
        "      init.normal(m.weight.data, 1.0, 0.02)\n",
        "      init.constant(m.bias.data, 0.0)\n",
        "\n",
        "def weights_init_kaiming(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    elif isinstance(m, nn.Linear) != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    elif isinstance(m, nn.BatchNorm2d) != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_orthogonal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.orthogonal(m.weight.data, gain=1)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.orthogonal(m.weight.data, gain=1)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal(m.weight.data, 1.0, 0.02)\n",
        "        init.constant(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def init_weights(net, init_type='normal'):\n",
        "    #print('initialization method [%s]' % init_type)\n",
        "    if init_type == 'normal':\n",
        "        net.apply(weights_init_normal)\n",
        "    elif init_type == 'xavier':\n",
        "        net.apply(weights_init_xavier)\n",
        "    elif init_type == 'kaiming':\n",
        "        net.apply(weights_init_kaiming)\n",
        "    elif init_type == 'orthogonal':\n",
        "        net.apply(weights_init_orthogonal)\n",
        "    else:\n",
        "        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "\n",
        "\n",
        "class _GridAttentionBlockND(nn.Module):\n",
        "    def __init__(self, in_channels, gating_channels, inter_channels=None, dimension=3,\n",
        "                 sub_sample_factor=(2,2,2)):\n",
        "        super(_GridAttentionBlockND, self).__init__()\n",
        "\n",
        "        assert dimension in [2, 3]\n",
        "\n",
        "        # Downsampling rate for the input featuremap\n",
        "        if isinstance(sub_sample_factor, tuple): self.sub_sample_factor = sub_sample_factor\n",
        "        elif isinstance(sub_sample_factor, list): self.sub_sample_factor = tuple(sub_sample_factor)\n",
        "        else: self.sub_sample_factor = tuple([sub_sample_factor]) * dimension\n",
        "\n",
        "        # Default parameter set\n",
        "        self.dimension = dimension\n",
        "        self.sub_sample_kernel_size = self.sub_sample_factor\n",
        "\n",
        "        # Number of channels (pixel dimensions)\n",
        "        self.in_channels = in_channels\n",
        "        self.gating_channels = gating_channels\n",
        "        self.inter_channels = inter_channels\n",
        "\n",
        "        if self.inter_channels is None:\n",
        "            self.inter_channels = in_channels // 2\n",
        "            if self.inter_channels == 0:\n",
        "                self.inter_channels = 1\n",
        "\n",
        "        if dimension == 3:\n",
        "            conv_nd = nn.Conv3d\n",
        "            bn = nn.BatchNorm3d\n",
        "            self.upsample_mode = 'trilinear'\n",
        "        elif dimension == 2:\n",
        "            conv_nd = nn.Conv2d\n",
        "            bn = nn.BatchNorm2d\n",
        "            self.upsample_mode = 'bilinear'\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "        # Output transform\n",
        "        self.W = nn.Sequential(\n",
        "            conv_nd(in_channels=self.in_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0),\n",
        "            bn(self.in_channels),\n",
        "        )\n",
        "\n",
        "        # Theta^T * x_ij + Phi^T * gating_signal + bias\n",
        "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                             kernel_size=self.sub_sample_kernel_size, stride=self.sub_sample_factor, padding=0, bias=False)\n",
        "        self.phi = conv_nd(in_channels=self.gating_channels, out_channels=self.inter_channels,\n",
        "                           kernel_size=1, stride=1, padding=0, bias=True)\n",
        "        self.psi = conv_nd(in_channels=self.inter_channels, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "        # Initialise weights\n",
        "        for m in self.children():\n",
        "            init_weights(m, init_type='xavier')\n",
        "\n",
        "        # Define the operation\n",
        "        self.operation_function = self._concatenation\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, g):\n",
        "        '''\n",
        "        :param x: (b, c, t, h, w)\n",
        "        :param g: (b, g_d)\n",
        "        :return:\n",
        "        '''\n",
        "        output = self.operation_function(x, g)\n",
        "        return output\n",
        "\n",
        "    def _concatenation(self, x, g):\n",
        "        input_size = x.size()\n",
        "        batch_size = input_size[0]\n",
        "        assert batch_size == g.size(0)\n",
        "\n",
        "        # theta => (b, c, t, h, w) -> (b, i_c, t, h, w) -> (b, i_c, thw)\n",
        "        # phi   => (b, g_d) -> (b, i_c)\n",
        "        theta_x = self.theta(x)\n",
        "        theta_x_size = theta_x.size()\n",
        "\n",
        "        # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n",
        "        #  Relu(theta_x + phi_g + bias) -> f = (b, i_c, thw) -> (b, i_c, t/s1, h/s2, w/s3)\n",
        "        phi_g = F.upsample(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n",
        "        f = F.relu(theta_x + phi_g, inplace=True)\n",
        "\n",
        "        #  psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n",
        "        sigm_psi_f = F.sigmoid(self.psi(f))\n",
        "\n",
        "        # upsample the attentions and multiply\n",
        "        sigm_psi_f = F.upsample(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode)\n",
        "        y = sigm_psi_f.expand_as(x) * x\n",
        "        W_y = self.W(y)\n",
        "\n",
        "        return W_y, sigm_psi_f\n",
        "\n",
        "\n",
        "class GridAttentionBlock2D(_GridAttentionBlockND):\n",
        "    def __init__(self, in_channels, gating_channels, inter_channels=None,\n",
        "                 sub_sample_factor=(2,2)):\n",
        "        super(GridAttentionBlock2D, self).__init__(in_channels,\n",
        "                                                   inter_channels=inter_channels,\n",
        "                                                   gating_channels=gating_channels,\n",
        "                                                   dimension=2,\n",
        "                                                   sub_sample_factor=sub_sample_factor,\n",
        "                                                   )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4MyicHiiWIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Defining UNET Architecture ###\n",
        "class unet(nn.Module):\n",
        "\n",
        "  def contracting_block(self, in_channels, out_channels):\n",
        "    block = nn.Sequential(nn.Conv2d(in_channels,out_channels, 3, padding = 1), nn.ReLU(), nn.BatchNorm2d(out_channels),\n",
        "                          nn.Conv2d(out_channels, out_channels, 3, padding  = 1), nn.ReLU(), nn.BatchNorm2d(out_channels))\n",
        "    \n",
        "    return block\n",
        "\n",
        "  def expansive_block(self, in_channels, mid_channel, out_channels):\n",
        "    block = nn.Sequential(nn.Conv2d(in_channels, mid_channel, 3, padding = 1), nn.ReLU(), nn.BatchNorm2d(mid_channel),\n",
        "                          nn.Conv2d(mid_channel, mid_channel, 3, padding = 1), nn.ReLU(), nn.BatchNorm2d(mid_channel),\n",
        "                          nn.ConvTranspose2d(mid_channel, out_channels, 3, 2,padding = 1,output_padding = 1))\n",
        "    return block\n",
        "\n",
        "  def final_block(self, in_channels, mid_channel, out_channels):\n",
        "    block = nn.Sequential(nn.Conv2d(in_channels, mid_channel, 3, padding = 1), nn.ReLU(), nn.BatchNorm2d(mid_channel),\n",
        "                          nn.Conv2d(mid_channel, mid_channel, 3, padding = 1), nn.ReLU(), nn.BatchNorm2d(mid_channel), \n",
        "                          nn.Conv2d(mid_channel, out_channels, 3, padding =1), nn.Sigmoid())\n",
        "    return block\n",
        "\n",
        "\n",
        "\n",
        "  def __init__(self, in_channel, out_channel):\n",
        "    super(unet, self).__init__()\n",
        "\n",
        "    self.encode1 = self.contracting_block(in_channel, 64)\n",
        "    self.maxpool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.encode2 = self.contracting_block(64,128)\n",
        "    self.maxpool2 = nn.MaxPool2d(2)\n",
        "    self.encode3 = self.contracting_block(128,256)\n",
        "    self.maxpool3 = nn.MaxPool2d(2)\n",
        "\n",
        "    self.bottleneck = self.expansive_block(256,512,256)\n",
        "\n",
        "    self.decode3 = self.expansive_block(512,256,128)\n",
        "    self.decode2 = self.expansive_block(256,128,64)\n",
        "\n",
        "    self.ag1 = GridAttentionBlock2D(256,256)\n",
        "    self.ag2 = GridAttentionBlock2D(128,128)\n",
        "    self.ag3 = GridAttentionBlock2D(64,64)\n",
        "    \n",
        "    self.final_layer = self.final_block(128,64,out_channel)\n",
        "\n",
        "  \n",
        "  def crop_and_concat(self, upsampled, bypass):\n",
        "      c = (bypass.size()[2] - upsampled.size()[2]) // 2\n",
        "      bypass = F.pad(bypass, (-c, -c, -c, -c))\n",
        "      bypass = F.upsample(bypass, (upsampled.size(2), upsampled.size(3)), mode = 'bilinear')\n",
        "      return torch.cat((upsampled, bypass), 1)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    encode1 = self.encode1(x)\n",
        "    maxpool1 = self.maxpool1(encode1)\n",
        "    encode2 = self.encode2(maxpool1)\n",
        "    maxpool2 = self.maxpool2(encode2)\n",
        "    encode3 = self.encode3(maxpool2)\n",
        "    maxpool3 = self.maxpool3(encode3)\n",
        "\n",
        "    bottleneck = self.bottleneck(maxpool3)\n",
        "\n",
        "    gate1 = self.ag1(encode3, bottleneck)\n",
        "    encode3 = encode3 + gate1[0]\n",
        "\n",
        "    decode3 = self.crop_and_concat(bottleneck, encode3)\n",
        "    cat_layer2 = self.decode3(decode3)\n",
        "    \n",
        "    gate2 = self.ag2(encode2, cat_layer2)\n",
        "    encode2 = encode2 + gate2[0]\n",
        "\n",
        "    decode2 = self.crop_and_concat(cat_layer2, encode2)\n",
        "    cat_layer1 = self.decode2(decode2)\n",
        "    \n",
        "    gate3 = self.ag3(encode1, cat_layer1)\n",
        "    encode1 = encode1 + gate3[0]\n",
        "\n",
        "    decode1 = self.crop_and_concat(cat_layer1, encode1)\n",
        "    final = self.final_layer(decode1)\n",
        "\n",
        "    return final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdLAG4G5iWK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdaptiveBatchNorm2d(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n",
        "        super(AdaptiveBatchNorm2d, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(num_features, eps, momentum, affine)\n",
        "        self.a = nn.Parameter(torch.FloatTensor(1, 1, 1, 1))\n",
        "        self.b = nn.Parameter(torch.FloatTensor(1, 1, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.a * x + self.b * self.bn(x)\n",
        "\n",
        "\n",
        "class vgg_type(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(vgg_type, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 64, [1,3], padding=[0,1], bias = False)\n",
        "    self.norm1 = AdaptiveBatchNorm2d(64)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "    self.lrelu = nn.LeakyReLU(0.2, inplace = True)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(64,64,[1,3], padding = [0,1], dilation = 1, bias = False)\n",
        "    self.conv3 = nn.Conv2d(64,64,[1,3], padding = [0,2], dilation = 2, bias = False)\n",
        "    self.conv4 = nn.Conv2d(64,64,[1,3], padding = [0,4], dilation = 4, bias = False)\n",
        "    self.conv5 = nn.Conv2d(64,64,[1,3], padding = [0,8], dilation = 8, bias = False)\n",
        "    self.conv6 = nn.Conv2d(64,64,[1,3], padding = [0,16], dilation = 16, bias = False)\n",
        "    self.conv7 = nn.Conv2d(64,64,[1,3], padding = [0,32], dilation = 32, bias = False)\n",
        "    self.conv8 = nn.Conv2d(64,64,[1,3], padding = [0,64], dilation = 64, bias = False)\n",
        "    self.conv9 = nn.Conv2d(64,64,[1,3], padding = [0,128], dilation = 128, bias = False)\n",
        "    self.conv10 = nn.Conv2d(64,64,[1,3], padding = [0,256], dilation = 256, bias = False)\n",
        "    self.conv11 = nn.Conv2d(64,64,[1,3], padding = [0,512], dilation = 512, bias = False)\n",
        "    self.conv12 = nn.Conv2d(64,64,[1,3], padding = [0,1024], dilation = 1024, bias = False)\n",
        "    self.conv13 = nn.Conv2d(64,64,[1,3], padding = [0,2048], dilation = 2048, bias = False)\n",
        "    self.conv14 = nn.Conv2d(64,64,[1,3], padding = [0,1], bias = False)\n",
        " \n",
        "    self.norm2 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm3 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm4 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm5 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm6 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm7 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm8 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm9 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm10 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm11 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm12 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm13 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm14 = AdaptiveBatchNorm2d(64)\n",
        "\n",
        "    self.final = nn.Conv2d(64,1, [1,1])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.norm1(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.norm2(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.norm3(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    x = self.norm4(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv5(x)\n",
        "    x = self.norm5(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv6(x)\n",
        "    x = self.norm6(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv7(x)\n",
        "    x = self.norm7(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv8(x)\n",
        "    x = self.norm8(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv9(x)\n",
        "    x = self.norm9(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv10(x)\n",
        "    x = self.norm10(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv11(x)\n",
        "    x = self.norm11(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv12(x)\n",
        "    x = self.norm12(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv13(x)\n",
        "    x = self.norm13(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv14(x)\n",
        "    x = self.norm14(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.final(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpTNfyZgiWNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,ids):\n",
        "    self.ids = ids\n",
        "    self.approx_inputs = []\n",
        "    self.approx_targets = []\n",
        "    self.detailed_inputs = []\n",
        "    self.detailed_targets = []\n",
        "    self.length = len(self.ids) // 16\n",
        "\n",
        "    self.random_ids = random.sample(self.ids, self.length)\n",
        "\n",
        " #   self.mean = mean\n",
        " #   self.std = std\n",
        " #   self.mean_target = mean_target\n",
        " #   self.std_target = std_target\n",
        "    \n",
        "    for id_ in self.random_ids:\n",
        "      input_location = 'noisy_dataset/noisy_trainset_56spk_wav/' + id_\n",
        "      target_location = 'clean_dataset/' + id_\n",
        "      \n",
        "      y, sr = librosa.load(input_location)\n",
        "      approx_input, detailed_input = pywt.dwt(y, 'db1')\n",
        "      approx_input = torch.from_numpy(approx_input)\n",
        "      detailed_input = torch.from_numpy(detailed_input)\n",
        "      approx_input = approx_input.unsqueeze_(0)\n",
        "      detailed_input = detailed_input.unsqueeze_(0)\n",
        "      self.approx_inputs.append(approx_input)\n",
        "      self.detailed_inputs.append(detailed_input)\n",
        "\n",
        "      y, sr = librosa.load(target_location)\n",
        "      approx_target, detailed_target = pywt.dwt(y, 'db1')\n",
        "      approx_target = torch.from_numpy(approx_target)\n",
        "      detailed_target = torch.from_numpy(detailed_target)\n",
        "      approx_target = approx_target.unsqueeze_(0)\n",
        "      detailed_target = detailed_target.unsqueeze_(0)\n",
        "      self.approx_targets.append(approx_target)\n",
        "      self.detailed_targets.append(detailed_target)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    approx_input = self.approx_inputs[index]\n",
        "    approx_target = self.approx_targets[index]\n",
        "\n",
        "    detailed_input = self.detailed_inputs[index]\n",
        "    detailed_target = self.detailed_targets[index]\n",
        "\n",
        "    return approx_input, detailed_input, approx_target, detailed_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwLwTglBiWPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ids = os.listdir('clean_dataset')\n",
        "dataset = AudioDataset(ids)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 1, shuffle = True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = 1, shuffle = False)\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wez0irC8HZkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Training Functions ###\n",
        "\n",
        "def train(dataloader, approx_model, detailed_model, approx_optimizer, detailed_optimizer, criterion):\n",
        "  detailed_model.train()\n",
        "  approx_model.train()\n",
        "  approx_train_losses.append(0)\n",
        "  detailed_train_losses.append(0)\n",
        "  progbar = tqdm_notebook(total = len(dataloader), desc = 'Approx_train')\n",
        "  progbar1 = tqdm_notebook(total = len(dataloader), desc = 'Detailed_train')\n",
        "\n",
        "  for i, (approx_input, detailed_input, approx_target, detailed_target) in enumerate(dataloader):\n",
        "    detailed_optimizer.zero_grad()\n",
        "    approx_optimizer.zero_grad()\n",
        "\n",
        "    approx_input, detailed_input, approx_target, detailed_target = Variable(approx_input.unsqueeze_(0).cuda()), Variable(detailed_input.unsqueeze_(0).cuda()), Variable(approx_target.unsqueeze_(0).cuda()), Variable(detailed_target.unsqueeze_(0).cuda())\n",
        "\n",
        "    approx_output = approx_model(approx_input)\n",
        "    detailed_output = detailed_model(detailed_input)\n",
        "\n",
        "    approx_error = criterion(approx_output, approx_target)\n",
        "    detailed_error = criterion(detailed_output, detailed_target)\n",
        "\n",
        "    approx_error.backward()\n",
        "    detailed_error.backward()\n",
        "\n",
        "    approx_optimizer.step()\n",
        "    detailed_optimizer.step()\n",
        "\n",
        "    approx_train_losses[-1] = approx_train_losses[-1] + approx_error.data\n",
        "    detailed_train_losses[-1] = detailed_train_losses[-1] + detailed_error.data\n",
        "\n",
        "    progbar.set_description('Approx Train (loss=%.4f)' % (approx_train_losses[-1]/(i+1)))\n",
        "    progbar.update(1)\n",
        "\n",
        "    progbar1.set_description('Detailed Train (loss = %.4f)' % (detailed_train_losses[-1]/(i+1)))\n",
        "    progbar1.update(1)\n",
        "\n",
        "  approx_train_losses[-1] = approx_train_losses[-1]/len(dataloader)\n",
        "  detailed_train_losses[-1] = detailed_train_losses[-1]/len(dataloader)\n",
        "\n",
        "def val(dataloader, approx_model, detailed_model, criterion):\n",
        "  \n",
        "  global approx_best_loss, detailed_best_loss\n",
        "  progbar = tqdm_notebook(total = len(dataloader), desc = 'approx val')\n",
        "  progbar1 = tqdm_notebook(total = len(dataloader), desc = 'detailed val')\n",
        "\n",
        "  approx_model.eval()\n",
        "  detailed_model.eval()\n",
        "\n",
        "  approx_val_losses.append(0)\n",
        "  detailed_val_losses.append(0)\n",
        "\n",
        "  for i, (approx_input, detailed_input, approx_target, detailed_target) in enumerate(dataloader):\n",
        "    approx_input, detailed_input, approx_target, detailed_target = Variable(approx_input.unsqueeze_(0).cuda()), Variable(detailed_input.unsqueeze_(0).cuda()), Variable(approx_target.unsqueeze_(0).cuda()), Variable(detailed_target.unsqueeze_(0).cuda())\n",
        "\n",
        "    approx_output = approx_model(approx_input)\n",
        "    approx_error = criterion(approx_output, approx_target)\n",
        "\n",
        "    detailed_output = detailed_model(detailed_input)\n",
        "    detailed_error = criterion(detailed_output, detailed_target)\n",
        "\n",
        "    approx_val_losses[-1] = approx_val_losses[-1] + approx_error.data\n",
        "    detailed_val_losses[-1] = detailed_val_losses[-1] + detailed_error.data\n",
        "\n",
        "    progbar.set_description('Approx Val (loss = %.4f)' % (approx_val_losses[-1] /(i+1)))\n",
        "    progbar.update(1)\n",
        "\n",
        "    progbar1.set_description('Detailed Val (loss = %.4f)' % (detailed_val_losses[-1] /(i+1)))\n",
        "    progbar1.update(1)\n",
        "\n",
        "  approx_val_losses[-1] = approx_val_losses[-1]/(len(dataloader))\n",
        "  detailed_val_losses[-1] = detailed_val_losses[-1]/(len(dataloader))\n",
        "\n",
        "  if approx_best_loss > approx_val_losses[-1]:\n",
        "    approx_best_loss = approx_val_losses[-1]\n",
        "    print('Approx Model SAVING....')\n",
        "    state = {'model' : approx_model}\n",
        "\n",
        "    torch.save(state, 'dwt_approx_model_best' + '.ckpt.t7')\n",
        "\n",
        "  if detailed_best_loss > detailed_val_losses[-1]:\n",
        "    detailed_best_loss = detailed_val_losses[-1]\n",
        "    print('Detailed Model SAVING....')\n",
        "    state = {'model' : detailed_model}\n",
        "\n",
        "    torch.save(state, 'dwt_detailed_model_best' + '.ckpt.t7')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuQbHzKzHb7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#approx_model = vgg_type().cuda()\n",
        "#detailed_model = vgg_type().cuda()\n",
        "checkpoints = torch.load('audio_load.ckpt.t7')\n",
        "approx_model = checkpoints['approx_model']\n",
        "detailed_model = checkpoints['detailed_model']\n",
        "num = checkpoints['epoch']\n",
        "criterion = torch.nn.L1Loss(reduction = 'sum')\n",
        "\n",
        "approx_train_losses = []\n",
        "approx_val_losses = []\n",
        "\n",
        "detailed_train_losses = []\n",
        "detailed_val_losses = []\n",
        "\n",
        "epochs = 1816 - num\n",
        "\n",
        "lrs = [1e-4, 1e-4, 1e-4, 1e-4, 1e-4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QcLgG98HdPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "approx_best_loss = checkpoints['approx_best_loss']\n",
        "detailed_best_loss = checkpoints['detailed_best_loss']\n",
        "\n",
        "approx_optimizer = torch.optim.Adam(approx_model.parameters(), lr = lrs[4])\n",
        "detailed_optimizer = torch.optim.Adam(detailed_model.parameters(), lr = lrs[4])\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train(train_loader, approx_model, detailed_model, approx_optimizer, detailed_optimizer, criterion)\n",
        "  val(val_loader, approx_model, detailed_model, criterion)\n",
        "  checkpoints = {'approx_model' : approx_model, 'detailed_model' : detailed_model, 'epoch' : epoch, 'approx_best_loss' : approx_best_loss, 'detailed_best_loss' : detailed_best_loss}\n",
        "  torch.save(checkpoints, 'audio_load.ckpt.t7')\n",
        "\n",
        "  print('Epoch : %d/%d' % (epoch+1, epochs))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}