{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom_speech_denoising.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIKU2fwpDfAva3dxAW73ZV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/advaitkumar3107/Speech-Denoising-Using-Deep-Learning/blob/master/VGG-16_type_without_dwt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YTFM9E9xuNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "import os\n",
        "import glob\n",
        "import sys\n",
        "import scipy\n",
        "import random\n",
        "from PIL import Image\n",
        "from torch.nn import init\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import librosa\n",
        "import librosa.display\n",
        "from tqdm import tqdm_notebook\n",
        "from scipy import signal\n",
        "from scipy.io.wavfile import read, write\n",
        "from numpy.fft import fft, ifft\n",
        "from google.colab import drive\n",
        "from torch.autograd import Variable\n",
        "from IPython.display import Audio\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/My\\ Drive/sample_audio_dataset\n",
        "\n",
        "torch.cuda.manual_seed(7)\n",
        "torch.manual_seed(7)\n",
        "np.random.seed(7)\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AShRAvVxz07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        init.normal(m.weight.data, 0.0, 0.02)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        init.normal(m.weight.data, 0.0, 0.02)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        init.normal(m.weight.data, 1.0, 0.02)\n",
        "        init.constant(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_xavier(m):\n",
        "  classname = m.__class__.__name__\n",
        "  #print(classname)\n",
        "  if isinstance(m, nn.Conv2d):\n",
        "      init.xavier_normal(m.weight.data, gain=1)\n",
        "  elif isinstance(m, nn.Linear):\n",
        "      init.xavier_normal(m.weight.data, gain=1)\n",
        "  elif isinstance(m, nn.BatchNorm2d):\n",
        "      init.normal(m.weight.data, 1.0, 0.02)\n",
        "      init.constant(m.bias.data, 0.0)\n",
        "\n",
        "def weights_init_kaiming(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    elif isinstance(m, nn.Linear) != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    elif isinstance(m, nn.BatchNorm2d) != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def weights_init_orthogonal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.orthogonal(m.weight.data, gain=1)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.orthogonal(m.weight.data, gain=1)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal(m.weight.data, 1.0, 0.02)\n",
        "        init.constant(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def init_weights(net, init_type='normal'):\n",
        "    #print('initialization method [%s]' % init_type)\n",
        "    if init_type == 'normal':\n",
        "        net.apply(weights_init_normal)\n",
        "    elif init_type == 'xavier':\n",
        "        net.apply(weights_init_xavier)\n",
        "    elif init_type == 'kaiming':\n",
        "        net.apply(weights_init_kaiming)\n",
        "    elif init_type == 'orthogonal':\n",
        "        net.apply(weights_init_orthogonal)\n",
        "    else:\n",
        "        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "\n",
        "\n",
        "class _GridAttentionBlockND(nn.Module):\n",
        "    def __init__(self, in_channels, gating_channels, inter_channels=None, dimension=3,\n",
        "                 sub_sample_factor=(2,2,2)):\n",
        "        super(_GridAttentionBlockND, self).__init__()\n",
        "\n",
        "        assert dimension in [2, 3]\n",
        "\n",
        "        # Downsampling rate for the input featuremap\n",
        "        if isinstance(sub_sample_factor, tuple): self.sub_sample_factor = sub_sample_factor\n",
        "        elif isinstance(sub_sample_factor, list): self.sub_sample_factor = tuple(sub_sample_factor)\n",
        "        else: self.sub_sample_factor = tuple([sub_sample_factor]) * dimension\n",
        "\n",
        "        # Default parameter set\n",
        "        self.dimension = dimension\n",
        "        self.sub_sample_kernel_size = self.sub_sample_factor\n",
        "\n",
        "        # Number of channels (pixel dimensions)\n",
        "        self.in_channels = in_channels\n",
        "        self.gating_channels = gating_channels\n",
        "        self.inter_channels = inter_channels\n",
        "\n",
        "        if self.inter_channels is None:\n",
        "            self.inter_channels = in_channels // 2\n",
        "            if self.inter_channels == 0:\n",
        "                self.inter_channels = 1\n",
        "\n",
        "        if dimension == 3:\n",
        "            conv_nd = nn.Conv3d\n",
        "            bn = nn.BatchNorm3d\n",
        "            self.upsample_mode = 'trilinear'\n",
        "        elif dimension == 2:\n",
        "            conv_nd = nn.Conv2d\n",
        "            bn = nn.BatchNorm2d\n",
        "            self.upsample_mode = 'bilinear'\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "        # Output transform\n",
        "        self.W = nn.Sequential(\n",
        "            conv_nd(in_channels=self.in_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0),\n",
        "            bn(self.in_channels),\n",
        "        )\n",
        "\n",
        "        # Theta^T * x_ij + Phi^T * gating_signal + bias\n",
        "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
        "                             kernel_size=self.sub_sample_kernel_size, stride=self.sub_sample_factor, padding=0, bias=False)\n",
        "        self.phi = conv_nd(in_channels=self.gating_channels, out_channels=self.inter_channels,\n",
        "                           kernel_size=1, stride=1, padding=0, bias=True)\n",
        "        self.psi = conv_nd(in_channels=self.inter_channels, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "        # Initialise weights\n",
        "        for m in self.children():\n",
        "            init_weights(m, init_type='kaiming')\n",
        "\n",
        "        # Define the operation\n",
        "        self.operation_function = self._concatenation\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, g):\n",
        "        '''\n",
        "        :param x: (b, c, t, h, w)\n",
        "        :param g: (b, g_d)\n",
        "        :return:\n",
        "        '''\n",
        "        output = self.operation_function(x, g)\n",
        "        return output\n",
        "\n",
        "    def _concatenation(self, x, g):\n",
        "        input_size = x.size()\n",
        "        batch_size = input_size[0]\n",
        "        assert batch_size == g.size(0)\n",
        "\n",
        "        # theta => (b, c, t, h, w) -> (b, i_c, t, h, w) -> (b, i_c, thw)\n",
        "        # phi   => (b, g_d) -> (b, i_c)\n",
        "        theta_x = self.theta(x)\n",
        "        theta_x_size = theta_x.size()\n",
        "\n",
        "        # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n",
        "        #  Relu(theta_x + phi_g + bias) -> f = (b, i_c, thw) -> (b, i_c, t/s1, h/s2, w/s3)\n",
        "        phi_g = F.upsample(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n",
        "        f = F.relu(theta_x + phi_g, inplace=True)\n",
        "\n",
        "        #  psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n",
        "        sigm_psi_f = F.sigmoid(self.psi(f))\n",
        "\n",
        "        # upsample the attentions and multiply\n",
        "        sigm_psi_f = F.upsample(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode)\n",
        "        y = sigm_psi_f.expand_as(x) * x\n",
        "        W_y = self.W(y)\n",
        "\n",
        "        return W_y, sigm_psi_f\n",
        "\n",
        "\n",
        "class GridAttentionBlock2D(_GridAttentionBlockND):\n",
        "    def __init__(self, in_channels, gating_channels, inter_channels=None,\n",
        "                 sub_sample_factor=(2,2)):\n",
        "        super(GridAttentionBlock2D, self).__init__(in_channels,\n",
        "                                                   inter_channels=inter_channels,\n",
        "                                                   gating_channels=gating_channels,\n",
        "                                                   dimension=2,\n",
        "                                                   sub_sample_factor=sub_sample_factor,\n",
        "                                                   )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RDH67wAxz4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Defining UNET Architecture ###\n",
        "class unet(nn.Module):\n",
        "\n",
        "  def contracting_block(self, in_channels, out_channels):\n",
        "    block = nn.Sequential(nn.Conv2d(in_channels,out_channels, 3, padding = 1), nn.ReLU(), nn.BatchNorm2d(out_channels),\n",
        "                          nn.Conv2d(out_channels, out_channels, 3, padding  = 1), nn.ReLU(), nn.BatchNorm2d(out_channels))\n",
        "    \n",
        "    return block\n",
        "\n",
        "  def expansive_block(self, in_channels, mid_channel, out_channels):\n",
        "    block = nn.Sequential(nn.Conv2d(in_channels, mid_channel, 3, padding = 1), nn.ReLU(), nn.BatchNorm2d(mid_channel),\n",
        "                          nn.Conv2d(mid_channel, mid_channel, 3, padding = 1), nn.ReLU(), nn.BatchNorm2d(mid_channel),\n",
        "                          nn.ConvTranspose2d(mid_channel, out_channels, 3, 2,padding = 1,output_padding = 1))\n",
        "    return block\n",
        "\n",
        "  def final_block(self, in_channels, mid_channel, out_channels):\n",
        "    block = nn.Sequential(nn.Conv2d(in_channels, mid_channel, 3, padding = 1), nn.ReLU(), nn.BatchNorm2d(mid_channel),\n",
        "                          nn.Conv2d(mid_channel, mid_channel, 3, padding = 1), nn.ReLU(), nn.BatchNorm2d(mid_channel), \n",
        "                          nn.Conv2d(mid_channel, out_channels, 3, padding =1), nn.Sigmoid())\n",
        "    return block\n",
        "\n",
        "\n",
        "\n",
        "  def __init__(self, in_channel, out_channel):\n",
        "    super(unet, self).__init__()\n",
        "\n",
        "    self.encode1 = self.contracting_block(in_channel, 64)\n",
        "    self.maxpool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.encode2 = self.contracting_block(64,128)\n",
        "    self.maxpool2 = nn.MaxPool2d(2)\n",
        "    self.encode3 = self.contracting_block(128,256)\n",
        "    self.maxpool3 = nn.MaxPool2d(2)\n",
        "\n",
        "    self.bottleneck = self.expansive_block(256,512,256)\n",
        "\n",
        "    self.decode3 = self.expansive_block(512,256,128)\n",
        "    self.decode2 = self.expansive_block(256,128,64)\n",
        "\n",
        "    self.ag1 = GridAttentionBlock2D(256,256)\n",
        "    self.ag2 = GridAttentionBlock2D(128,128)\n",
        "    self.ag3 = GridAttentionBlock2D(64,64)\n",
        "    \n",
        "    self.final_layer = self.final_block(128,64,out_channel)\n",
        "\n",
        "  \n",
        "  def crop_and_concat(self, upsampled, bypass):\n",
        "      c = (bypass.size()[2] - upsampled.size()[2]) // 2\n",
        "      bypass = F.pad(bypass, (-c, -c, -c, -c))\n",
        "      bypass = F.upsample(bypass, (upsampled.size(2), upsampled.size(3)), mode = 'bilinear')\n",
        "      return torch.cat((upsampled, bypass), 1)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    encode1 = self.encode1(x)\n",
        "    maxpool1 = self.maxpool1(encode1)\n",
        "    encode2 = self.encode2(maxpool1)\n",
        "    maxpool2 = self.maxpool2(encode2)\n",
        "    encode3 = self.encode3(maxpool2)\n",
        "    maxpool3 = self.maxpool3(encode3)\n",
        "\n",
        "    bottleneck = self.bottleneck(maxpool3)\n",
        "\n",
        "    gate1 = self.ag1(encode3, bottleneck)\n",
        "    encode3 = encode3 + gate1[0]\n",
        "\n",
        "    decode3 = self.crop_and_concat(bottleneck, encode3)\n",
        "    cat_layer2 = self.decode3(decode3)\n",
        "    \n",
        "    gate2 = self.ag2(encode2, cat_layer2)\n",
        "    encode2 = encode2 + gate2[0]\n",
        "\n",
        "    decode2 = self.crop_and_concat(cat_layer2, encode2)\n",
        "    cat_layer1 = self.decode2(decode2)\n",
        "    \n",
        "    gate3 = self.ag3(encode1, cat_layer1)\n",
        "    encode1 = encode1 + gate3[0]\n",
        "\n",
        "    decode1 = self.crop_and_concat(cat_layer1, encode1)\n",
        "    final = self.final_layer(decode1)\n",
        "\n",
        "    return final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrlCI8oqxz9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdaptiveBatchNorm2d(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n",
        "        super(AdaptiveBatchNorm2d, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(num_features, eps, momentum, affine)\n",
        "        self.a = nn.Parameter(torch.FloatTensor(1, 1, 1, 1))\n",
        "        self.b = nn.Parameter(torch.FloatTensor(1, 1, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.a * x + self.b * self.bn(x)\n",
        "\n",
        "\n",
        "class vgg_type(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(vgg_type, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 64, [1,3], padding=[0,1], bias = False)\n",
        "    self.norm1 = AdaptiveBatchNorm2d(64)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "    self.lrelu = nn.LeakyReLU(0.2, inplace = True)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(64,64,[1,3], padding = [0,1], dilation = 1, bias = False)\n",
        "    self.conv3 = nn.Conv2d(64,64,[1,3], padding = [0,2], dilation = 2, bias = False)\n",
        "    self.conv4 = nn.Conv2d(64,64,[1,3], padding = [0,4], dilation = 4, bias = False)\n",
        "    self.conv5 = nn.Conv2d(64,64,[1,3], padding = [0,8], dilation = 8, bias = False)\n",
        "    self.conv6 = nn.Conv2d(64,64,[1,3], padding = [0,16], dilation = 16, bias = False)\n",
        "    self.conv7 = nn.Conv2d(64,64,[1,3], padding = [0,32], dilation = 32, bias = False)\n",
        "    self.conv8 = nn.Conv2d(64,64,[1,3], padding = [0,64], dilation = 64, bias = False)\n",
        "    self.conv9 = nn.Conv2d(64,64,[1,3], padding = [0,128], dilation = 128, bias = False)\n",
        "    self.conv10 = nn.Conv2d(64,64,[1,3], padding = [0,256], dilation = 256, bias = False)\n",
        "    self.conv11 = nn.Conv2d(64,64,[1,3], padding = [0,512], dilation = 512, bias = False)\n",
        "    self.conv12 = nn.Conv2d(64,64,[1,3], padding = [0,1024], dilation = 1024, bias = False)\n",
        "    self.conv13 = nn.Conv2d(64,64,[1,3], padding = [0,2048], dilation = 2048, bias = False)\n",
        "    self.conv14 = nn.Conv2d(64,64,[1,3], padding = [0,1], bias = False)\n",
        " \n",
        "    self.norm2 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm3 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm4 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm5 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm6 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm7 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm8 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm9 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm10 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm11 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm12 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm13 = AdaptiveBatchNorm2d(64)\n",
        "    self.norm14 = AdaptiveBatchNorm2d(64)\n",
        "\n",
        "    self.final = nn.Conv2d(64,1, [1,1])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.norm1(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.norm2(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.norm3(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    x = self.norm4(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv5(x)\n",
        "    x = self.norm5(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv6(x)\n",
        "    x = self.norm6(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv7(x)\n",
        "    x = self.norm7(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv8(x)\n",
        "    x = self.norm8(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv9(x)\n",
        "    x = self.norm9(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv10(x)\n",
        "    x = self.norm10(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv11(x)\n",
        "    x = self.norm11(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv12(x)\n",
        "    x = self.norm12(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv13(x)\n",
        "    x = self.norm13(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.conv14(x)\n",
        "    x = self.norm14(x)\n",
        "    x = self.lrelu(x)\n",
        "\n",
        "    x = self.final(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZCX5_-Px0Ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,ids):\n",
        "    self.ids = ids\n",
        "    self.inputs = []\n",
        "    self.targets = []\n",
        "    self.length = len(self.ids) // 16\n",
        "\n",
        "    self.random_ids = random.sample(self.ids, self.length)\n",
        "\n",
        " #   self.mean = mean\n",
        " #   self.std = std\n",
        " #   self.mean_target = mean_target\n",
        " #   self.std_target = std_target\n",
        "    \n",
        "    for id_ in self.random_ids:\n",
        "      input_location = 'noisy_dataset/noisy_trainset_56spk_wav/' + id_\n",
        "      target_location = 'clean_dataset/' + id_\n",
        "      \n",
        "      y, sr = librosa.load(input_location)\n",
        "      D = y\n",
        "      D = torch.from_numpy(D)\n",
        "      input_ = D.unsqueeze_(0)\n",
        "      self.inputs.append(input_)\n",
        "\n",
        "      y, sr = librosa.load(target_location)\n",
        "      D = torch.from_numpy(y)\n",
        "      target = D.unsqueeze_(0)\n",
        "      target = input_ - target\n",
        "      self.targets.append(target)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    input_ = self.inputs[index]\n",
        "    target = self.targets[index]\n",
        "\n",
        "    return input_, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXb4aLwOx0Fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ids = os.listdir('clean_dataset')\n",
        "dataset = AudioDataset(ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5HyYYc0x0L3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O93fWgLPx0UO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 1, shuffle = True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = 1, shuffle = False)\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go7W2q_hx0Rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Training Functions ###\n",
        "\n",
        "def train(dataloader, model, optimizer, criterion):\n",
        "  model.train()\n",
        "  train_losses.append(0)\n",
        "  progbar = tqdm_notebook(total = len(dataloader), desc = 'Train')\n",
        "\n",
        "\n",
        "  for i, (inputs,targets) in enumerate(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    inputs, targets = Variable(inputs.unsqueeze_(0).cuda()), Variable(targets.unsqueeze_(0).cuda())\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    targets = inputs - targets\n",
        "    error = criterion(outputs, targets)\n",
        "\n",
        "    error.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_losses[-1] = train_losses[-1] + error.data\n",
        "    progbar.set_description('Train (loss=%.4f)' % (train_losses[-1]/(i+1)))\n",
        "    progbar.update(1)\n",
        "\n",
        "  train_losses[-1] = train_losses[-1]/len(dataloader)\n",
        "\n",
        "\n",
        "def val(dataloader, model, criterion):\n",
        "  \n",
        "  global best_loss\n",
        "  progbar = tqdm_notebook(total = len(dataloader), desc = 'UNet Val')\n",
        "  model.eval()\n",
        "\n",
        "  val_losses.append(0)\n",
        "\n",
        "  for i, (inputs, targets) in enumerate(dataloader):\n",
        "    inputs, targets = Variable(inputs.unsqueeze_(0).cuda()), Variable(targets.unsqueeze_(0).cuda())\n",
        "    \n",
        "    outputs = model(inputs)\n",
        "    targets = inputs - targets\n",
        "    error = criterion(outputs, targets)\n",
        "\n",
        "    val_losses[-1] = val_losses[-1] + error.data\n",
        "    progbar.set_description('Val (loss = %.4f)' % (val_losses[-1] /(i+1)))\n",
        "    progbar.update(1)\n",
        "\n",
        "  val_losses[-1] = val_losses[-1]/(len(dataloader))\n",
        "\n",
        "  if best_loss > val_losses[-1]:\n",
        "    best_loss = val_losses[-1]\n",
        "    print('SAVING....')\n",
        "    state = {'model' : model}\n",
        "\n",
        "    torch.save(state, 'noise_raw' + '.ckpt.t7')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTRUfmUbx0Pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = vgg_type().cuda()\n",
        "checkpoints = torch.load('noise_load.ckpt.t7')\n",
        "model = checkpoints['model']\n",
        "criterion = torch.nn.L1Loss(reduction = 'sum')\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "epochs = 1422 - checkpoints['epoch']\n",
        "\n",
        "lrs = [1e-4, 1e-4, 1e-4, 1e-4, 1e-4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv6bns9fsXLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#best_loss = 1e5\n",
        "best_loss = checkpoints['best_loss']\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  if epoch == 0:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = lrs[0])\n",
        "\n",
        "  if epoch == 30:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = lrs[1])\n",
        "\n",
        "  if epoch == 60:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = lrs[2])\n",
        "\n",
        "  if epoch == 90:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = lrs[3])\n",
        "\n",
        "  if epoch == 150:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = lrs[4])\n",
        "    \n",
        "  train(train_loader, model, optimizer, criterion)\n",
        "  val(val_loader, model, criterion)\n",
        "  checkpoints = {'model' : model, 'epoch' : epoch, 'best_loss' : best_loss}\n",
        "  torch.save(checkpoints, 'noise_load.ckpt.t7')\n",
        "\n",
        "  print('Epoch : %d/%d' % (epoch+1, epochs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6O9wfWFxz7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoints = torch.load('audio_raw_updated.ckpt.t7')\n",
        "model = checkpoints['model']\n",
        "y, sr = librosa.load('advait.ogg')\n",
        "y = torch.from_numpy(y)\n",
        "y = y.unsqueeze_(0).unsqueeze_(0).unsqueeze_(0).cuda()\n",
        "output = model(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsIJ6mUqsqXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import IPython\n",
        "IPython.display.Audio('advait.ogg', rate = sr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0ndJc9DJJS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = output.squeeze_(0).squeeze_(0).detach().cpu().numpy()\n",
        "IPython.display.Audio(output, rate = sr)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}